@inproceedings{yan_framework_2019,
	address = {Minneapolis, Minnesota},
	title = {A {Framework} for {Decoding} {Event}-{Related} {Potentials} from {Text}},
	url = {https://www.aclweb.org/anthology/W19-2910},
	doi = {10.18653/v1/W19-2910},
	abstract = {We propose a novel framework for modeling event-related potentials (ERPs) collected during reading that couples pre-trained convolutional decoders with a language model. Using this framework, we compare the abilities of a variety of existing and novel sentence processing models to reconstruct ERPs. We find that modern contextual word embeddings underperform surprisal-based models but that, combined, the two outperform either on its own.},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yan, Shaorong and White, Aaron Steven},
	month = jun,
	year = {2019},
	pages = {86--92}
}